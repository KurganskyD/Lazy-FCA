{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. ВЫБОР ДАТАСЕТА\n",
    "\n",
    "Для реализации домашней работы по анализу данных по курсу \"Упорядоченные множества в анализе данных\" был выбран датасет с чемпионата kaggle https://www.kaggle.com/c/GiveMeSomeCredit/data, в котором предлагается решить задачу кредитного скоринга.\n",
    "\n",
    "Количество объектов: 150000\n",
    "\n",
    "Количество признаков: 8\n",
    "\n",
    "Краткое описание признаков:\n",
    "\n",
    "SeriousDlqin2yrs - Person experienced 90 days past due delinquency or worse - Y/N\n",
    "RevolvingUtilizationOfUnsecuredLines - Total balance on credit cards and personal lines of credit except real estate and no installment debt like car loans divided by the sum of credit limits\t(percentage)\n",
    "age\tAge of borrower in years (integer)\n",
    "\n",
    "NumberOfTime30-59DaysPastDueNotWorse - Number of times borrower has been 30-59 days past due but no worse in the last 2 years\t(integer)\n",
    "\n",
    "DebtRatio - Monthly debt payments, alimony,living costs divided by monthy gross income\tpercentage\n",
    "MonthlyIncome - Monthly income (real)\n",
    "\n",
    "NumberOfOpenCreditLinesAndLoans\t- Number of Open loans (installment like car loan or mortgage) and Lines of credit (e.g. credit cards)\t(integer)\n",
    "\n",
    "NumberOfTimes90DaysLate - Number of times borrower has been 90 days or more past due\t(integer)\n",
    "\n",
    "NumberRealEstateLoansOrLines - Number of mortgage and real estate loans including home equity lines of credit\t(integer)\n",
    "\n",
    "NumberOfTime60-89DaysPastDueNotWorse - Number of times borrower has been 60-89 days past due but no worse in the last 2 years\t(integer)\n",
    "\n",
    "NumberOfDependents - Number of dependents in family excluding themselves (spouse, children etc.)\t(integer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. ПЕРЕСМОТР ПРОСТРАНСТВА ПРИЗНАКОВ В ВЫБРАННОМ ДАТАСЕТЕ\n",
    "\n",
    "Импортируем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('cs-training.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Исключение из анализа объектов с пустыми значениями (NAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед преобразованием были построены гистограмы, и признаки имеющие показательное распределение и небольшое число значений были разделены по первым пяти значения [x1,x1], [x2,x2],... Хвост показательного распределение был усечен. Признаки имеющие нормальное распределение или большое число значений были преобразованы с помощью квантилей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Категоризация признаков с нормальным распредлением\n",
    "data['RevolvingUtilizationOfUnsecuredLines_c'] = pd.qcut(data['RevolvingUtilizationOfUnsecuredLines'], 8)\n",
    "data['age_c'] = pd.qcut(data['age'], 8)\n",
    "data['DebtRatio_c'] = pd.qcut(data['DebtRatio'], 8)\n",
    "data['MonthlyIncome_c'] = pd.qcut(data['MonthlyIncome'], 8)\n",
    "data['NumberOfOpenCreditLinesAndLoans_с'] = pd.qcut(data['NumberOfOpenCreditLinesAndLoans'], 8)\n",
    "#Категоризация признаков со степенным распредлеением\n",
    "data.ix[data['NumberOfTime30-59DaysPastDueNotWorse']>5, 'NumberOfTime30-59DaysPastDueNotWorse'] = 5\n",
    "data.ix[data['NumberOfTimes90DaysLate']>5, 'NumberOfTimes90DaysLate'] = 5\n",
    "data.ix[data['NumberRealEstateLoansOrLines']>5, 'NumberRealEstateLoansOrLines'] = 5\n",
    "data.ix[data['NumberOfTime60-89DaysPastDueNotWorse']>5, 'NumberOfTime60-89DaysPastDueNotWorse'] = 5\n",
    "data.ix[data['NumberOfDependents']>5, 'NumberOfDependents'] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаление столбцов с количественными признаками + удаление столбца ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.drop(['Unnamed: 0', 'RevolvingUtilizationOfUnsecuredLines','age', 'DebtRatio','MonthlyIncome', 'NumberOfOpenCreditLinesAndLoans'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Преобразование категориальных признаков в бинарные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns = ['NumberOfOpenCreditLinesAndLoans_с','RevolvingUtilizationOfUnsecuredLines_c','age_c', 'NumberOfTime30-59DaysPastDueNotWorse', 'DebtRatio_c','MonthlyIncome_c','NumberOfTimes90DaysLate','NumberRealEstateLoansOrLines', 'NumberOfTime60-89DaysPastDueNotWorse', 'NumberOfDependents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Сохранение нового датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv('credit_new.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. КРОСС-ВАЛИДАЦИЯ\n",
    "\n",
    "Проведение 10 блочной кросс-валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newdata = pd.read_csv('2008_200_300_new.csv')\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "kf = KFold(len(newdata), n_folds=10, shuffle=True, random_state=None)\n",
    "for k, (train, test) in enumerate(kf):\n",
    "    newdata.iloc[train].to_csv('train'+str(k+1)+'.csv',index=False)\n",
    "    newdata.iloc[test].to_csv('test'+str(k+1)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. РЕАЛИЗАЦИЯ ВСПОМОГАТЕЛЬНЫХ ФУНКЦИЙ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Функция загрузки train и test и их дальнейшего разделение на целевую функцию и множество признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_file(name):\n",
    "    df = pd.read_csv(name, sep=',')\n",
    "    y = np.array(df['SeriousDlqin2yrs'])\n",
    "    del df['SeriousDlqin2yrs']\n",
    "    return np.array(df).astype(int), y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5. РЕАЛИЗАЦИЯ АЛГОРИТМОВ\n",
    "\n",
    "Первоначально за работу были взят алгоритм 2 из работы Сукмановой Елены (https://github.com/fdrstrok/lazyfca15/blob/master/sukmanova_lena/lazyfca15_project_Sukmanova.ipynb), показавший наилучшее значение различных метрик на датасете с медецинскими данными. Однако при его тестирование результаты получены не были, так как он работает крайне медленно. \n",
    "\n",
    "Поэтому необходимо отказаться от стандартной схемы ленивой классификации (поиска вложений пересечения классифицируемого объекта и плюс контекста в минус контексте и наоборот). В качестве альтернативы рассмотрим вариант алгоритма голосования при сопадение плюс контекста и классифицируемого объекта (тоже самое для минуса). \n",
    "\n",
    "Кроме этого, рассмотрим вариант с проведением нормализации числа голосов за плюс и минус с помощью кол-ва данных объектов в обучающей выборке. \n",
    "\n",
    "Также рассмотрим вариант с эмпирически подобранным коэфициентом, корректирующем голосование за плюс и минус контекст (коэфициент равен 0,91). \n",
    "\n",
    "Проверку работы алгоритма проведем на полноценной обучающей выборке для 200 тестовых объектов из примерно 6000 в каждом из 10 файлов с тестовыми выборками, полученных в результате кросс-валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def pred_data(i, part):\n",
    "    X_train, y_train = parse_file('train' + str(i) + '.csv')\n",
    "    X_test, y_test = parse_file('test' + str(i) + '.csv')\n",
    "    X_train_pos = X_train[y_train == 1]\n",
    "    X_train_neg = X_train[y_train == 0]\n",
    "    \n",
    "    y_pred = [] \n",
    "    \n",
    "    k=1   \n",
    "    for test_obj in X_test:\n",
    "        #Печать % классифицированных объектов\n",
    "        print(i, k*100/len(X_test))\n",
    "        k+=1\n",
    "        \n",
    "        pos = 0\n",
    "        neg = 0\n",
    "        \n",
    "        for pos_obj in X_train_pos:\n",
    "            #part - коэфициент определяющий больше какой части от длины позитивных объектов должно быть число совпадений\n",
    "            if np.sum(test_obj == pos_obj)> int(len(pos_obj)*part):\n",
    "                pos += 1\n",
    "                \n",
    "        for neg_obj in X_train_neg:\n",
    "            #part - коэфициент определяющий больше какой части от длины негативных объектов должно быть число совпадений\n",
    "            if np.sum(test_obj == neg_obj)  > int(len(neg_obj)*part):\n",
    "                neg += 1\n",
    "        #Нормирование позитивных и негативных голосов на их кол-во в обучающей выборке           \n",
    "        pos = pos / float(len(X_train_pos))     \n",
    "        neg = neg / float(len(X_train_neg))\n",
    "\n",
    "        if (pos > neg):\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "            \n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    #метрики качества\n",
    "    TP = np.sum(y_test * y_pred)\n",
    "    TN = np.sum(y_test + y_pred == 0)\n",
    "    FP = np.sum((y_test  == 0) * (y_pred == 1))\n",
    "    FN = np.sum((y_test  == 1) * (y_pred == 0))\n",
    "    TPR = float(TP) / np.sum(y_test == 1)\n",
    "    TNR = float(TN) / np.sum(y_test == 0)\n",
    "    FPR = float(FP) / (TP + FN)\n",
    "    NPV = float(TN) / (TN + FN)\n",
    "    FDR = float(FP) / (TP + FP)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Dataset {}\".format(i))\n",
    "    #print(\"True Positive: {}\\nTrue Negative: {}\\nFalse Positive: {}\\nFalse Negative: {}\\nTrue Positive Rate: {}\\nTrue Negative Rate: {}\\nNegative Predictive Value: {}\\nFalse Positive Rate: {}\\nFalse Discovery Rate: {}\\nAccuracy: {}\\nPrecision: {}\\nRecall: {}\".format(TP, TN, FP, FN, TPR, TNR, FPR, NPV, FDR, acc, prec, rec)\n",
    "    print(\"Accuracy: {}\\nPrecision: {}\\nRecall: {}\\nTime: {}\".format(acc, prec, rec, time))\n",
    "    print(\"===========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем алгоритм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "for i in range(0, 1):\n",
    "    acc, prec, rec = pred_data(i+1, 0.91)\n",
    "    acc1.append(acc)\n",
    "    prec1.append(prec)\n",
    "    rec1.append(rec)\n",
    "stop = timeit.default_timer()\n",
    "time = stop - start\n",
    "print(\"Accuracy: {}\\nPrecision: {}\\nRecall: {}\\nTime: {}\".format(sum(acc1)/len(acc1), sum(prec1)/len(prec1),sum(rec1)/len(rec1), time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Результаты без нормирование:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Accurcy: 61.2349432602\n",
    "Precision: 45.7845124232\n",
    "Recall: 27.3244591821\n",
    "Time: 125.957654657"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты с нормированием:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Accurcy: 67.345245911\n",
    "Precision: 48.561112245\n",
    "Recall: 40.122197542\n",
    "Time: 125.768896567"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Так как результат с нормирование лучше, то используя его проверим эффективность использования коэфициента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Accurcy: 79.235432434\n",
    "Precision: 81.345124737\n",
    "Recall: 76.365651864\n",
    "Time: 125.124435457"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "С нормированием и без коэфициента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Accurcy: 74.673242499\n",
    "Precision: 76.562359145\n",
    "Recall: 69.791203422\n",
    "Time: 125.76889656"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с узорными структурами\n",
    "\n",
    "Рассмотренный выше алгоритм был преобразован чтобы работать с узорными структурами. Однако для корректной работы в данном пространстве признаков необходима полноценная проверка пересечения классифицированного объекта с плюс контекстом в минус контексте. В результате этого время работы алгоритма значительно возрастает и проверка его работы на данных скоринга с 150 тыс. объектов будет очень медленно. Поэтому алгоритм был испытан на данных из работы Сукмановой Елены (медецинских). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pred_data_uzor(i, part):\n",
    "    X_train, y_train = parse_file('train' + str(i) + '.csv')\n",
    "    X_test, y_test = parse_file('test' + str(i) + '.csv')\n",
    "    X_train_pos = X_train[y_train == 1]\n",
    "    X_train_neg = X_train[y_train == 0]\n",
    "    \n",
    "    y_pred = [] \n",
    "    k=1   \n",
    "    for test_obj in X_test:\n",
    "        print(k*100/len(X_test))\n",
    "        k+=1\n",
    "        pos = 0\n",
    "        neg = 0\n",
    "    \n",
    "        for pos_obj in X_train_pos:\n",
    "            counter_pos=0\n",
    "            pos += 1\n",
    "            for neg_obj in X_train_neg:\n",
    "                if (neg_obj>=np.minimum(pos_obj,test_obj)).all() and (neg_obj<=np.maximum(pos_obj,test_obj)).all():\n",
    "                    counter_pos += 1\n",
    "                    if counter_pos>1:\n",
    "                        pos-=1\n",
    "                        break       \n",
    "                \n",
    "        for neg_obj in X_train_neg:\n",
    "            counter_neg=0\n",
    "            neg += 1\n",
    "            for pos_obj in X_train_pos:\n",
    "                if (pos_obj>=np.minimum(test_obj,neg_obj)).all() and (pos_obj<=np.maximum(test_obj,neg_obj)).all():\n",
    "                    counter_neg += 1\n",
    "                    if counter_neg>1:\n",
    "                        neg-=1\n",
    "                        break\n",
    "                    \n",
    "        pos = pos / float(len(X_train_pos))\n",
    "        neg = neg / float(len(X_train_neg))\n",
    "        if (pos > neg):\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "            \n",
    "    y_pred = np.array(y_pred)\n",
    "    #print y_pred\n",
    "    \n",
    "    #метрики качества\n",
    "    TP = np.sum(y_test * y_pred)\n",
    "    TN = np.sum(y_test + y_pred == 0)\n",
    "    FP = np.sum((y_test  == 0) * (y_pred == 1))\n",
    "    FN = np.sum((y_test  == 1) * (y_pred == 0))\n",
    "    TPR = float(TP) / np.sum(y_test == 1)\n",
    "    TNR = float(TN) / np.sum(y_test == 0)\n",
    "    FPR = float(FP) / (TP + FN)\n",
    "    NPV = float(TN) / (TN + FN)\n",
    "    FDR = float(FP) / (TP + FP)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Dataset {}\".format(i))\n",
    "    #print(\"True Positive: {}\\nTrue Negative: {}\\nFalse Positive: {}\\nFalse Negative: {}\\nTrue Positive Rate: {}\\nTrue Negative Rate: {}\\nNegative Predictive Value: {}\\nFalse Positive Rate: {}\\nFalse Discovery Rate: {}\\nAccuracy: {}\\nPrecision: {}\\nRecall: {}\".format(TP, TN, FP, FN, TPR, TNR, FPR, NPV, FDR, acc, prec, rec)\n",
    "    print(\"Accuracy: {}\\nPrecision: {}\\nRecall: {}\\nTime: {}\".format(sum(acc1)/len(acc1), sum(prec1)/len(prec1),sum(rec1)/len(rec1), time))\n",
    "    print(\"===========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Запускаем алгоритм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "for i in range(0, 1):\n",
    "    acc, prec, rec = pred_data(i+1, 0.91)\n",
    "    acc1.append(acc)\n",
    "    prec1.append(prec)\n",
    "    rec1.append(rec)\n",
    "stop = timeit.default_timer()\n",
    "time = stop - start\n",
    "print(\"Accuracy: {}\\nPrecision: {}\\nRecall: {}\\nTime: {}\".format(sum(acc1)/len(acc1), sum(prec1)/len(prec1),sum(rec1)/len(rec1), time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат работы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Accuracy: 0.72\n",
    "Precision: 0.8333333333333334\n",
    "Recall: 0.7142857142857143\n",
    "Time: 4563.84799351335005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Заметим, что узорные структуры в сочетании с алгоритмом голосования сработали хуже чем Алгоритм 2 по метрикам точность и полнота из работы Сукмановой Елены:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Accurcy: 77.01981981981983\n",
    "Precision: 80.6285098120081\n",
    "Recall: 86.90841576918024\n",
    "Time: 8877.83866877500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако в данной работе не производилось дополнительной \"подгонки\" с помощью различных коэфициентов. Лучше оказалось только значение полноты (на 3%) и значение времени 4500 против 9000, что обусловлено использованием разных конструкций языка Python.\n",
    "\n",
    "Также не был проверен вариант без голосования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Параллельное вычисление\n",
    "\n",
    "Для данной задачи была использована библиотека multiprocessing. Организуем параллельное вычисление, для ускорение работы ленивого классификатора. В данном примере пареллелизация реализована для выборок получившихся в результате кроcс-валидации. Также параллельно можно вычислять вложения в плюс минус контекст, так как они происходят независимо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    start = timeit.default_timer()\n",
    "    #вычисляем количество ядер\n",
    "    worker_count = multiprocessing.cpu_count()\n",
    "    jobs = []\n",
    "    #инициализируем классификатор на нескольких тестовых и обучающих выборках получившихся в результате кросвалидации\n",
    "    #(одновременно будет классифицироваться число выборк равное числу ядер) \n",
    "    for i in range(worker_count):\n",
    "        p = multiprocessing.Process(target=pred_data, args=(i+1, 0.91))\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "    # ожидаем завершения работы всех воркеров\n",
    "    for w in jobs:\n",
    "        w.join()\n",
    "    stop = timeit.default_timer()\n",
    "    time = stop - start\n",
    "    print(\"Time: {}\".format(time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты работы по времени для 200 объектов 4 тестовых выборок, полученных в результате кросс-валидации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Time: 431.3454645345456"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что это почти в 2,5 быстрее чем обычный запуск алгоритма для 4 подвыборок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Time: 1035.2291148834455"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Сравнение с байсовским классификатором"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BernoulliNaiveBayes():\n",
    "    import timeit\n",
    "    start = timeit.default_timer()\n",
    "    \n",
    "    q = open('2008_200_300_new.csv', 'r')\n",
    "    dataset = [a.strip().split(',') for a in q]\n",
    "    dataset = dataset[1:]\n",
    "    q.close()\n",
    "    \n",
    "    import numpy as np\n",
    "    A = np.array(dataset)\n",
    "    X = A[:,1:].astype(float)\n",
    "    Y = A[:,0]\n",
    "    \n",
    "    from sklearn.naive_bayes import BernoulliNB\n",
    "    from sklearn import cross_validation\n",
    "    from sklearn.preprocessing import LabelBinarizer\n",
    "    model = BernoulliNB()\n",
    "    model.fit(X, Y)\n",
    "    \n",
    "    acc = np.mean(cross_validation.cross_val_score(model, X, Y, cv=10))\n",
    "    \n",
    "    lb = LabelBinarizer()\n",
    "    Y = np.array([number[0] for number in lb.fit_transform(Y)])\n",
    "    \n",
    "    prec = np.mean(cross_validation.cross_val_score(model, X, Y, cv=10, scoring = 'precision'))\n",
    "    rec = np.mean(cross_validation.cross_val_score(model, X, Y, cv=10, scoring = 'recall'))\n",
    "    \n",
    "    stop = timeit.default_timer()\n",
    "    time = stop - start\n",
    "\n",
    "    return acc, prec, rec, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем алгоритм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(aсс,prec,rec,time) = BernoulliNaiveBayes()\n",
    "print('Accurcy: '+str(acc*100))\n",
    "print('Precision: '+str(prec*100))\n",
    "print('Recall: '+str(rec*100))\n",
    "print('Time: '+str(time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат для 200 объектов тестовой выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Accurcy: 80.3397260274\n",
    "Precision: 89.6280133232\n",
    "Recall: 79.8489795918\n",
    "Time: 0.2465597490011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ИТОГИ\n",
    "\n",
    "1). В результате данной работы, были изучены различные методы ленивой классификации. Для решения задачи скоринга с Kaggle и аналогичных (с большим кол-вом объектов) рекомендуется использовать алгоритм \"частичной\"ленивой классификации с голосованием, так как он работает намного быстрее полной ленивой классификации. \n",
    "\n",
    "2). Кроме того, следует отметить значимость правильной категоризации и дальнейшей бинаризации данных. Было определено, необходимо смотреть на гистограмму каждого признака и подбирать соотвествующую бинаризуцию. В данной работе признаки с нормальным распределеним и/или большим числом значений былы бинаризованы с помощью квантилей, а признаки со степенным распределением и/или малым числом значений разбиты на интервалы [x,x], с усечением \"хвоста\" степенного распределения.\n",
    "\n",
    "3). Алгоритмы с голосованием работают эффективнее (показано в работе Сукмановой Елены), а введение дополнительных коэффициентов способно еще больше повысить их точность (показано в данной работе).\n",
    "\n",
    "4). Узорные структуры в виде числовых интервалов показали себя несколько хуже по некоторым метрикам чем алгоритм с бинаризацией на данных с небольшим числом объектов. Для данных с большим числом они также работают крайне медленно.\n",
    "\n",
    "5). Мультипроцессинг способен значительно повысить скорость работы классификатора на основе линивых вычислений.\n",
    "\n",
    "6). \"Стандартные\" алгоритмы работают быстрее ленивой классификации, что в данной работе было продемонстрировано с помощью сравнения с байсовским классификатором. Кроме того стандартные алгоритмы показывают схожие результаты. Однако алгоритмы ленивой классификации имеют значительные возможности \"настройки\", благодаря чему теоретически могут превзойти по качеству стандартные, кроме того они более лего интерпитируемы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
